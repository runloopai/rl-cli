
# Meta details

Running `kata-collect-data.sh` version `3.20.0 (commit c980b6e191e174053681fb30817736e040554c10)` at `2025-09-09.17:30:46.291404024+0000`.

---

<details>
<summary>Runtime</summary>
<p>

Runtime is `/usr/local/bin/kata-runtime`.

# `kata-env`

<details>
<summary><tt>/usr/local/bin/kata-runtime kata-env</tt></summary>
<p>

```toml
[Kernel]
  Path = "/opt/kata/share/kata-containers/vmlinux-6.12.42-162"
  Parameters = "systemd.unit=kata-containers.target systemd.mask=systemd-networkd.service systemd.mask=systemd-networkd.socket scsi_mod.scan=none cgroup_no_v1=all systemd.unified_cgroup_hierarchy=1"

[Meta]
  Version = "1.0.27"

[Image]
  Path = "/opt/kata/share/kata-containers/kata-ubuntu-noble.image"

[Initrd]
  Path = ""

[Hypervisor]
  MachineType = "q35"
  Version = "QEMU emulator version 10.0.3 (kata-static)\nCopyright (c) 2003-2025 Fabrice Bellard and the QEMU Project developers"
  Path = "/opt/kata/bin/qemu-system-x86_64"
  BlockDeviceDriver = "virtio-scsi"
  EntropySource = "/dev/urandom"
  SharedFS = "virtio-fs"
  VirtioFSDaemon = "/opt/kata/libexec/virtiofsd"
  SocketPath = ""
  Msize9p = 8192
  MemorySlots = 10
  HotPlugVFIO = "no-port"
  ColdPlugVFIO = "no-port"
  PCIeRootPort = 0
  PCIeSwitchPort = 0
  Debug = false
  [Hypervisor.SecurityInfo]
    Rootless = false
    DisableSeccomp = false
    GuestHookPath = ""
    EnableAnnotations = ["enable_iommu", "virtio_fs_extra_args", "kernel_params"]
    ConfidentialGuest = false

[Runtime]
  Path = "/opt/kata/bin/kata-runtime"
  GuestSeLinuxLabel = ""
  Debug = false
  Trace = false
  DisableGuestSeccomp = true
  DisableNewNetNs = false
  SandboxCgroupOnly = false
  [Runtime.Config]
    Path = "/opt/kata/share/defaults/kata-containers/configuration-qemu.toml"
  [Runtime.Version]
    OCI = "1.2.0"
    [Runtime.Version.Version]
      Semver = "3.20.0"
      Commit = "c980b6e191e174053681fb30817736e040554c10"
      Major = 3
      Minor = 20
      Patch = 0

[Host]
  Kernel = "6.14.0-1012-aws"
  Architecture = "amd64"
  VMContainerCapable = true
  SupportVSocks = true
  [Host.Distro]
    Name = "Ubuntu"
    Version = "24.04"
  [Host.CPU]
    Vendor = "GenuineIntel"
    Model = "Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz"
    CPUs = 96
  [Host.Memory]
    Total = 197658616
    Free = 96573764
    Available = 147729108

[Agent]
  Debug = false
  Trace = false
```

</p>
</details>

---


</p>
</details>
<details>
<summary>Runtime config files</summary>
<p>


# Runtime config files


## Runtime default config files

```
/etc/kata-containers/configuration.toml
/opt/kata/share/defaults/kata-containers/configuration.toml
```

## Runtime config file contents

Config file `/etc/kata-containers/configuration.toml` not found
<details>
<summary><tt>cat "/opt/kata/share/defaults/kata-containers/configuration.toml"</tt></summary>
<p>

```toml
# Copyright (c) 2017-2019 Intel Corporation
# Copyright (c) 2021 Adobe Inc.
#
# SPDX-License-Identifier: Apache-2.0
#

# XXX: WARNING: this file is auto-generated.
# XXX:
# XXX: Source file: "config/configuration-qemu.toml.in"
# XXX: Project:
# XXX:   Name: Kata Containers
# XXX:   Type: kata

[hypervisor.qemu]
path = "/opt/kata/bin/qemu-system-x86_64"
kernel = "/opt/kata/share/kata-containers/vmlinux.container"
image = "/opt/kata/share/kata-containers/kata-containers.img"
# initrd = "/opt/kata/share/kata-containers/kata-containers-initrd.img"
machine_type = "q35"

# rootfs filesystem type:
#   - ext4 (default)
#   - xfs
#   - erofs
rootfs_type="ext4"

# Enable confidential guest support.
# Toggling that setting may trigger different hardware features, ranging
# from memory encryption to both memory and CPU-state encryption and integrity.
# The Kata Containers runtime dynamically detects the available feature set and
# aims at enabling the largest possible one, returning an error if none is
# available, or none is supported by the hypervisor.
#
# Known limitations:
# * Does not work by design:
#   - CPU Hotplug 
#   - Memory Hotplug
#   - NVDIMM devices
#
# Default false
# confidential_guest = true

# Choose AMD SEV-SNP confidential guests
# In case of using confidential guests on AMD hardware that supports both SEV
# and SEV-SNP, the following enables SEV-SNP guests. SEV guests are default.
# Default false
# sev_snp_guest = true

# Enable running QEMU VMM as a non-root user.
# By default QEMU VMM run as root. When this is set to true, QEMU VMM process runs as
# a non-root random user. See documentation for the limitations of this mode.
# rootless = true

# List of valid annotation names for the hypervisor
# Each member of the list is a regular expression, which is the base name
# of the annotation, e.g. "path" for io.katacontainers.config.hypervisor.path"
enable_annotations = ["enable_iommu", "virtio_fs_extra_args", "kernel_params"]

# List of valid annotations values for the hypervisor
# Each member of the list is a path pattern as described by glob(3).
# The default if not set is empty (all annotations rejected.)
# Your distribution recommends: ["/opt/kata/bin/qemu-system-x86_64"]
valid_hypervisor_paths = ["/opt/kata/bin/qemu-system-x86_64"]

# Optional space-separated list of options to pass to the guest kernel.
# For example, use `kernel_params = "vsyscall=emulate"` if you are having
# trouble running pre-2.15 glibc.
#
# WARNING: - any parameter specified here will take priority over the default
# parameter value of the same name used to start the virtual machine.
# Do not set values here unless you understand the impact of doing so as you
# may stop the virtual machine from booting.
# To see the list of default parameters, enable hypervisor debug, create a
# container and look for 'default-kernel-parameters' log entries.
kernel_params = "cgroup_no_v1=all systemd.unified_cgroup_hierarchy=1"

# Path to the firmware.
# If you want that qemu uses the default firmware leave this option empty
firmware = ""

# Path to the firmware volume.
# firmware TDVF or OVMF can be split into FIRMWARE_VARS.fd (UEFI variables
# as configuration) and FIRMWARE_CODE.fd (UEFI program image). UEFI variables
# can be customized per each user while UEFI code is kept same.
firmware_volume = ""

# Machine accelerators
# comma-separated list of machine accelerators to pass to the hypervisor.
# For example, `machine_accelerators = "nosmm,nosmbus,nosata,nopit,static-prt,nofw"`
machine_accelerators=""

# Qemu seccomp sandbox feature
# comma-separated list of seccomp sandbox features to control the syscall access.
# For example, `seccompsandbox= "on,obsolete=deny,spawn=deny,resourcecontrol=deny"`
# Note: "elevateprivileges=deny" doesn't work with daemonize option, so it's removed from the seccomp sandbox
# Another note: enabling this feature may reduce performance, you may enable
# /proc/sys/net/core/bpf_jit_enable to reduce the impact. see https://man7.org/linux/man-pages/man8/bpfc.8.html
#seccompsandbox="on,obsolete=deny,spawn=deny,resourcecontrol=deny"

# CPU features
# comma-separated list of cpu features to pass to the cpu
# For example, `cpu_features = "pmu=off,vmx=off"
cpu_features="pmu=off"

# Default number of vCPUs per SB/VM:
# unspecified or 0                --> will be set to 1
# < 0                             --> will be set to the actual number of physical cores
# > 0 <= number of physical cores --> will be set to the specified number
# > number of physical cores      --> will be set to the actual number of physical cores
default_vcpus = 1

# Default maximum number of vCPUs per SB/VM:
# unspecified or == 0             --> will be set to the actual number of physical cores or to the maximum number
#                                     of vCPUs supported by KVM if that number is exceeded
# > 0 <= number of physical cores --> will be set to the specified number
# > number of physical cores      --> will be set to the actual number of physical cores or to the maximum number
#                                     of vCPUs supported by KVM if that number is exceeded
# WARNING: Depending of the architecture, the maximum number of vCPUs supported by KVM is used when
# the actual number of physical cores is greater than it.
# WARNING: Be aware that this value impacts the virtual machine's memory footprint and CPU
# the hotplug functionality. For example, `default_maxvcpus = 240` specifies that until 240 vCPUs
# can be added to a SB/VM, but the memory footprint will be big. Another example, with
# `default_maxvcpus = 8` the memory footprint will be small, but 8 will be the maximum number of
# vCPUs supported by the SB/VM. In general, we recommend that you do not edit this variable,
# unless you know what are you doing.
# NOTICE: on arm platform with gicv2 interrupt controller, set it to 8.
default_maxvcpus = 0

# Bridges can be used to hot plug devices.
# Limitations:
# * Currently only pci bridges are supported
# * Until 30 devices per bridge can be hot plugged.
# * Until 5 PCI bridges can be cold plugged per VM.
#   This limitation could be a bug in qemu or in the kernel
# Default number of bridges per SB/VM:
# unspecified or 0   --> will be set to 1
# > 1 <= 5           --> will be set to the specified number
# > 5                --> will be set to 5
default_bridges = 1

# Default memory size in MiB for SB/VM.
# If unspecified then it will be set 2048 MiB.
default_memory = 2048
#
# Default memory slots per SB/VM.
# If unspecified then it will be set 10.
# This is will determine the times that memory will be hotadded to sandbox/VM.
#memory_slots = 10

# Default maximum memory in MiB per SB / VM
# unspecified or == 0           --> will be set to the actual amount of physical RAM
# > 0 <= amount of physical RAM --> will be set to the specified number
# > amount of physical RAM      --> will be set to the actual amount of physical RAM
default_maxmemory = 0

# The size in MiB will be plused to max memory of hypervisor.
# It is the memory address space for the NVDIMM device.
# If set block storage driver (block_device_driver) to "nvdimm",
# should set memory_offset to the size of block device.
# Default 0
#memory_offset = 0

# Specifies virtio-mem will be enabled or not.
# Please note that this option should be used with the command
# "echo 1 > /proc/sys/vm/overcommit_memory".
# Default false
#enable_virtio_mem = true

# Disable block device from being used for a container's rootfs.
# In case of a storage driver like devicemapper where a container's
# root file system is backed by a block device, the block device is passed
# directly to the hypervisor for performance reasons.
# This flag prevents the block device from being passed to the hypervisor,
# virtio-fs is used instead to pass the rootfs.
disable_block_device_use = false

# Shared file system type:
#   - virtio-fs (default)
#   - virtio-9p
#   - virtio-fs-nydus
#   - none
shared_fs = "virtio-fs"

# Path to vhost-user-fs daemon.
virtio_fs_daemon = "/opt/kata/libexec/virtiofsd"

# List of valid annotations values for the virtiofs daemon
# The default if not set is empty (all annotations rejected.)
# Your distribution recommends: ["/opt/kata/libexec/virtiofsd"]
valid_virtio_fs_daemon_paths = ["/opt/kata/libexec/virtiofsd"]

# Default size of DAX cache in MiB
virtio_fs_cache_size = 0

# Default size of virtqueues
virtio_fs_queue_size = 1024

# Extra args for virtiofsd daemon
#
# Format example:
#   ["--arg1=xxx", "--arg2=yyy"]
# Examples:
#   Set virtiofsd log level to debug : ["--log-level=debug"]
#
# see `virtiofsd -h` for possible options.
virtio_fs_extra_args = ["--thread-pool-size=1", "--announce-submounts"]

# Cache mode:
#
#  - never
#    Metadata, data, and pathname lookup are not cached in guest. They are
#    always fetched from host and any changes are immediately pushed to host.
#
#  - metadata
#    Metadata and pathname lookup are cached in guest and never expire.
#    Data is never cached in guest.
#
#  - auto
#    Metadata and pathname lookup cache expires after a configured amount of
#    time (default is 1 second). Data is cached while the file is open (close
#    to open consistency).
#
#  - always
#    Metadata, data, and pathname lookup are cached in guest and never expire.
virtio_fs_cache = "auto"

# Block storage driver to be used for the hypervisor in case the container
# rootfs is backed by a block device. This is virtio-scsi, virtio-blk
# or nvdimm.
block_device_driver = "virtio-scsi"

# aio is the I/O mechanism used by qemu
# Options:
#
#   - threads
#     Pthread based disk I/O.
#
#   - native
#     Native Linux I/O.
#
#   - io_uring
#     Linux io_uring API. This provides the fastest I/O operations on Linux, requires kernel>5.1 and
#     qemu >=5.0.
block_device_aio = "io_uring"

# Specifies cache-related options will be set to block devices or not.
# Default false
#block_device_cache_set = true

# Specifies cache-related options for block devices.
# Denotes whether use of O_DIRECT (bypass the host page cache) is enabled.
# Default false
#block_device_cache_direct = true

# Specifies cache-related options for block devices.
# Denotes whether flush requests for the device are ignored.
# Default false
#block_device_cache_noflush = true

# Enable iothreads (data-plane) to be used. This causes IO to be
# handled in a separate IO thread. This is currently only implemented
# for SCSI.
#
enable_iothreads = false

# Enable pre allocation of VM RAM, default false
# Enabling this will result in lower container density
# as all of the memory will be allocated and locked
# This is useful when you want to reserve all the memory
# upfront or in the cases where you want memory latencies
# to be very predictable
# Default false
#enable_mem_prealloc = true

# Enable huge pages for VM RAM, default false
# Enabling this will result in the VM memory
# being allocated using huge pages.
# This is useful when you want to use vhost-user network
# stacks within the container. This will automatically
# result in memory pre allocation
#enable_hugepages = true

# Enable vhost-user storage device, default false
# Enabling this will result in some Linux reserved block type
# major range 240-254 being chosen to represent vhost-user devices.
enable_vhost_user_store = false

# The base directory specifically used for vhost-user devices.
# Its sub-path "block" is used for block devices; "block/sockets" is
# where we expect vhost-user sockets to live; "block/devices" is where
# simulated block device nodes for vhost-user devices to live.
vhost_user_store_path = "/var/run/kata-containers/vhost-user"

# Enable vIOMMU, default false
# Enabling this will result in the VM having a vIOMMU device
# This will also add the following options to the kernel's
# command line: intel_iommu=on,iommu=pt
#enable_iommu = true

# Enable IOMMU_PLATFORM, default false
# Enabling this will result in the VM device having iommu_platform=on set
#enable_iommu_platform = true

# List of valid annotations values for the vhost user store path
# The default if not set is empty (all annotations rejected.)
# Your distribution recommends: ["/var/run/kata-containers/vhost-user"]
valid_vhost_user_store_paths = ["/var/run/kata-containers/vhost-user"]

# The timeout for reconnecting on non-server spdk sockets when the remote end goes away.
# qemu will delay this many seconds and then attempt to reconnect.
# Zero disables reconnecting, and the default is zero.
vhost_user_reconnect_timeout_sec = 0

# Enable file based guest memory support. The default is an empty string which
# will disable this feature. In the case of virtio-fs, this is enabled
# automatically and '/dev/shm' is used as the backing folder.
# This option will be ignored if VM templating is enabled.
#file_mem_backend = ""

# List of valid annotations values for the file_mem_backend annotation
# The default if not set is empty (all annotations rejected.)
# Your distribution recommends: [""]
valid_file_mem_backends = [""]

# -pflash can add image file to VM. The arguments of it should be in format
# of ["/path/to/flash0.img", "/path/to/flash1.img"]
pflashes = []

# This option changes the default hypervisor and kernel parameters
# to enable debug output where available.
#
# Default false
#enable_debug = true

# This option allows to add an extra HMP or QMP socket when `enable_debug = true`
#
# WARNING: Anyone with access to the extra socket can take full control of
# Qemu. This is for debugging purpose only and must *NEVER* be used in
# production.
#
# Valid values are :
# - "hmp"
# - "qmp"
# - "qmp-pretty" (same as "qmp" with pretty json formatting)
#
# If set to the empty string "", no extra monitor socket is added. This is
# the default.
#extra_monitor_socket = hmp

# Disable the customizations done in the runtime when it detects
# that it is running on top a VMM. This will result in the runtime
# behaving as it would when running on bare metal.
#
#disable_nesting_checks = true

# This is the msize used for 9p shares. It is the number of bytes
# used for 9p packet payload.
#msize_9p = 8192

# If false and nvdimm is supported, use nvdimm device to plug guest image.
# Otherwise virtio-block device is used.
#
# nvdimm is not supported when `confidential_guest = true`.
disable_image_nvdimm = false

# Enable hot-plugging of VFIO devices to a bridge-port, 
# root-port or switch-port. 
# The default setting is  "no-port"
#hot_plug_vfio = "root-port" 

# In a confidential compute environment hot-plugging can compromise
# security. 
# Enable cold-plugging of VFIO devices to a bridge-port, 
# root-port or switch-port. 
# The default setting is  "no-port", which means disabled. 
#cold_plug_vfio = "root-port" 

# Before hot plugging a PCIe device, you need to add a pcie_root_port device.
# Use this parameter when using some large PCI bar devices, such as Nvidia GPU
# The value means the number of pcie_root_port
# Default 0
#pcie_root_port = 2

# If vhost-net backend for virtio-net is not desired, set to true. Default is false, which trades off
# security (vhost-net runs ring0) for network I/O performance.
#disable_vhost_net = true

#
# Default entropy source.
# The path to a host source of entropy (including a real hardware RNG)
# /dev/urandom and /dev/random are two main options.
# Be aware that /dev/random is a blocking source of entropy.  If the host
# runs out of entropy, the VMs boot time will increase leading to get startup
# timeouts.
# The source of entropy /dev/urandom is non-blocking and provides a
# generally acceptable source of entropy. It should work well for pretty much
# all practical purposes.
#entropy_source= "/dev/urandom"

# List of valid annotations values for entropy_source
# The default if not set is empty (all annotations rejected.)
# Your distribution recommends: ["/dev/urandom","/dev/random",""]
valid_entropy_sources = ["/dev/urandom","/dev/random",""]

# Path to OCI hook binaries in the *guest rootfs*.
# This does not affect host-side hooks which must instead be added to
# the OCI spec passed to the runtime.
#
# You can create a rootfs with hooks by customizing the osbuilder scripts:
# https://github.com/kata-containers/kata-containers/tree/main/tools/osbuilder
#
# Hooks must be stored in a subdirectory of guest_hook_path according to their
# hook type, i.e. "guest_hook_path/{prestart,poststart,poststop}".
# The agent will scan these directories for executable files and add them, in
# lexicographical order, to the lifecycle of the guest container.
# Hooks are executed in the runtime namespace of the guest. See the official documentation:
# https://github.com/opencontainers/runtime-spec/blob/v1.0.1/config.md#posix-platform-hooks
# Warnings will be logged if any error is encountered while scanning for hooks,
# but it will not abort container execution.
#guest_hook_path = "/usr/share/oci/hooks"
#
# Use rx Rate Limiter to control network I/O inbound bandwidth(size in bits/sec for SB/VM).
# In Qemu, we use classful qdiscs HTB(Hierarchy Token Bucket) to discipline traffic.
# Default 0-sized value means unlimited rate.
#rx_rate_limiter_max_rate = 0
# Use tx Rate Limiter to control network I/O outbound bandwidth(size in bits/sec for SB/VM).
# In Qemu, we use classful qdiscs HTB(Hierarchy Token Bucket) and ifb(Intermediate Functional Block)
# to discipline traffic.
# Default 0-sized value means unlimited rate.
#tx_rate_limiter_max_rate = 0

# Set where to save the guest memory dump file.
# If set, when GUEST_PANICKED event occurred,
# guest memeory will be dumped to host filesystem under guest_memory_dump_path,
# This directory will be created automatically if it does not exist.
#
# The dumped file(also called vmcore) can be processed with crash or gdb.
#
# WARNING:
#   Dump guest’s memory can take very long depending on the amount of guest memory
#   and use much disk space.
#guest_memory_dump_path="/var/crash/kata"

# If enable paging.
# Basically, if you want to use "gdb" rather than "crash",
# or need the guest-virtual addresses in the ELF vmcore,
# then you should enable paging.
#
# See: https://www.qemu.org/docs/master/qemu-qmp-ref.html#Dump-guest-memory for details
#guest_memory_dump_paging=false

# Enable swap in the guest. Default false.
# When enable_guest_swap is enabled, insert a raw file to the guest as the swap device
# if the swappiness of a container (set by annotation "io.katacontainers.container.resource.swappiness")
# is bigger than 0.
# The size of the swap device should be
# swap_in_bytes (set by annotation "io.katacontainers.container.resource.swap_in_bytes") - memory_limit_in_bytes.
# If swap_in_bytes is not set, the size should be memory_limit_in_bytes.
# If swap_in_bytes and memory_limit_in_bytes is not set, the size should
# be default_memory.
#enable_guest_swap = true

# use legacy serial for guest console if available and implemented for architecture. Default false
#use_legacy_serial = true

# disable applying SELinux on the VMM process (default false)
disable_selinux=false

# disable applying SELinux on the container process
# If set to false, the type `container_t` is applied to the container process by default.
# Note: To enable guest SELinux, the guest rootfs must be CentOS that is created and built
# with `SELINUX=yes`.
# (default: true)
disable_guest_selinux=true


[factory]
# VM templating support. Once enabled, new VMs are created from template
# using vm cloning. They will share the same initial kernel, initramfs and
# agent memory by mapping it readonly. It helps speeding up new container
# creation and saves a lot of memory if there are many kata containers running
# on the same host.
#
# When disabled, new VMs are created from scratch.
#
# Note: Requires "initrd=" to be set ("image=" is not supported).
#
# Default false
#enable_template = true

# Specifies the path of template.
#
# Default "/run/vc/vm/template"
#template_path = "/run/vc/vm/template"

# The number of caches of VMCache:
# unspecified or == 0   --> VMCache is disabled
# > 0                   --> will be set to the specified number
#
# VMCache is a function that creates VMs as caches before using it.
# It helps speed up new container creation.
# The function consists of a server and some clients communicating
# through Unix socket.  The protocol is gRPC in protocols/cache/cache.proto.
# The VMCache server will create some VMs and cache them by factory cache.
# It will convert the VM to gRPC format and transport it when gets
# requestion from clients.
# Factory grpccache is the VMCache client.  It will request gRPC format
# VM and convert it back to a VM.  If VMCache function is enabled,
# kata-runtime will request VM from factory grpccache when it creates
# a new sandbox.
#
# Default 0
#vm_cache_number = 0

# Specify the address of the Unix socket that is used by VMCache.
#
# Default /var/run/kata-containers/cache.sock
#vm_cache_endpoint = "/var/run/kata-containers/cache.sock"

[agent.kata]
# If enabled, make the agent display debug-level messages.
# (default: disabled)
#enable_debug = true

# Enable agent tracing.
#
# If enabled, the agent will generate OpenTelemetry trace spans.
#
# Notes:
#
# - If the runtime also has tracing enabled, the agent spans will be
#   associated with the appropriate runtime parent span.
# - If enabled, the runtime will wait for the container to shutdown,
#   increasing the container shutdown time slightly.
#
# (default: disabled)
#enable_tracing = true

# Comma separated list of kernel modules and their parameters.
# These modules will be loaded in the guest kernel using modprobe(8).
# The following example can be used to load two kernel modules with parameters
#  - kernel_modules=["e1000e InterruptThrottleRate=3000,3000,3000 EEE=1", "i915 enable_ppgtt=0"]
# The first word is considered as the module name and the rest as its parameters.
# Container will not be started when:
#  * A kernel module is specified and the modprobe command is not installed in the guest
#    or it fails loading the module.
#  * The module is not available in the guest or it doesn't met the guest kernel
#    requirements, like architecture and version.
#
kernel_modules=[]

# Enable debug console.

# If enabled, user can connect guest OS running inside hypervisor
# through "kata-runtime exec <sandbox-id>" command

#debug_console_enabled = true

# Agent connection dialing timeout value in seconds
# (default: 45)
dial_timeout = 45

# Confidential Data Hub API timeout value in seconds
# (default: 50)
#cdh_api_timeout = 50

[runtime]
# If enabled, the runtime will log additional debug messages to the
# system log
# (default: disabled)
#enable_debug = true
#
# Internetworking model
# Determines how the VM should be connected to the
# the container network interface
# Options:
#
#   - macvtap
#     Used when the Container network interface can be bridged using
#     macvtap.
#
#   - none
#     Used when customize network. Only creates a tap device. No veth pair.
#
#   - tcfilter
#     Uses tc filter rules to redirect traffic from the network interface
#     provided by plugin to a tap interface connected to the VM.
#
internetworking_model="tcfilter"

# disable guest seccomp
# Determines whether container seccomp profiles are passed to the virtual
# machine and applied by the kata agent. If set to true, seccomp is not applied
# within the guest
# (default: true)
disable_guest_seccomp=true

# vCPUs pinning settings
# if enabled, each vCPU thread will be scheduled to a fixed CPU
# qualified condition: num(vCPU threads) == num(CPUs in sandbox's CPUSet)
# enable_vcpus_pinning = false

# Apply a custom SELinux security policy to the container process inside the VM.
# This is used when you want to apply a type other than the default `container_t`,
# so general users should not uncomment and apply it.
# (format: "user:role:type")
# Note: You cannot specify MCS policy with the label because the sensitivity levels and
# categories are determined automatically by high-level container runtimes such as containerd.
#guest_selinux_label="system_u:system_r:container_t"

# If enabled, the runtime will create opentracing.io traces and spans.
# (See https://www.jaegertracing.io/docs/getting-started).
# (default: disabled)
#enable_tracing = true

# Set the full url to the Jaeger HTTP Thrift collector.
# The default if not set will be "http://localhost:14268/api/traces"
#jaeger_endpoint = ""

# Sets the username to be used if basic auth is required for Jaeger.
#jaeger_user = ""

# Sets the password to be used if basic auth is required for Jaeger.
#jaeger_password = ""

# If enabled, the runtime will not create a network namespace for shim and hypervisor processes.
# This option may have some potential impacts to your host. It should only be used when you know what you're doing.
# `disable_new_netns` conflicts with `internetworking_model=tcfilter` and `internetworking_model=macvtap`. It works only
# with `internetworking_model=none`. The tap device will be in the host network namespace and can connect to a bridge
# (like OVS) directly.
# (default: false)
#disable_new_netns = true

# if enabled, the runtime will add all the kata processes inside one dedicated cgroup.
# The container cgroups in the host are not created, just one single cgroup per sandbox.
# The runtime caller is free to restrict or collect cgroup stats of the overall Kata sandbox.
# The sandbox cgroup path is the parent cgroup of a container with the PodSandbox annotation.
# The sandbox cgroup is constrained if there is no container type annotation.
# See: https://pkg.go.dev/github.com/kata-containers/kata-containers/src/runtime/virtcontainers#ContainerType
sandbox_cgroup_only=false

# If enabled, the runtime will attempt to determine appropriate sandbox size (memory, CPU) before booting the virtual machine. In
# this case, the runtime will not dynamically update the amount of memory and CPU in the virtual machine. This is generally helpful
# when a hardware architecture or hypervisor solutions is utilized which does not support CPU and/or memory hotplug.
# Compatibility for determining appropriate sandbox (VM) size:
# - When running with pods, sandbox sizing information will only be available if using Kubernetes >= 1.23 and containerd >= 1.6. CRI-O
#   does not yet support sandbox sizing annotations.
# - When running single containers using a tool like ctr, container sizing information will be available.
static_sandbox_resource_mgmt = true

# If specified, sandbox_bind_mounts identifieds host paths to be mounted (ro) into the sandboxes shared path.
# This is only valid if filesystem sharing is utilized. The provided path(s) will be bindmounted into the shared fs directory.
# If defaults are utilized, these mounts should be available in the guest at `/run/kata-containers/shared/containers/sandbox-mounts`
# These will not be exposed to the container workloads, and are only provided for potential guest services.
sandbox_bind_mounts=[]

# VFIO Mode
# Determines how VFIO devices should be be presented to the container.
# Options:
#
#  - vfio
#    Matches behaviour of OCI runtimes (e.g. runc) as much as
#    possible.  VFIO devices will appear in the container as VFIO
#    character devices under /dev/vfio.  The exact names may differ
#    from the host (they need to match the VM's IOMMU group numbers
#    rather than the host's)
#
#  - guest-kernel
#    This is a Kata-specific behaviour that's useful in certain cases.
#    The VFIO device is managed by whatever driver in the VM kernel
#    claims it.  This means it will appear as one or more device nodes
#    or network interfaces depending on the nature of the device.
#    Using this mode requires specially built workloads that know how
#    to locate the relevant device interfaces within the VM.
#
vfio_mode="guest-kernel"

# If enabled, the runtime will not create Kubernetes emptyDir mounts on the guest filesystem. Instead, emptyDir mounts will
# be created on the host and shared via virtio-fs. This is potentially slower, but allows sharing of files from host to guest.
disable_guest_empty_dir=false

# Enabled experimental feature list, format: ["a", "b"].
# Experimental features are features not stable enough for production,
# they may break compatibility, and are prepared for a big version bump.
# Supported experimental features:
# (default: [])
experimental=[]

# If enabled, user can run pprof tools with shim v2 process through kata-monitor.
# (default: false)
# enable_pprof = true

# Indicates the CreateContainer request timeout needed for the workload(s)
# It using guest_pull this includes the time to pull the image inside the guest
# Defaults to 60 second(s)  
# Note: The effective timeout is determined by the lesser of two values: runtime-request-timeout from kubelet config 
# (https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/#:~:text=runtime%2Drequest%2Dtimeout) and create_container_timeout. 
# In essence, the timeout used for guest pull=runtime-request-timeout<create_container_timeout?runtime-request-timeout:create_container_timeout.
create_container_timeout = 60

# Base directory of directly attachable network config.
# Network devices for VM-based containers are allowed to be placed in the
# host netns to eliminate as many hops as possible, which is what we
# called a "Directly Attachable Network". The config, set by special CNI
# plugins, is used to tell the Kata containers what devices are attached
# to the hypervisor.
# (default: /run/kata-containers/dans)
dan_conf = "/run/kata-containers/dans"
```

</p>
</details>
Config file `/usr/share/defaults/kata-containers/configuration.toml` not found

---


</p>
</details>
<details>
<summary>Containerd shim v2</summary>
<p>

Containerd shim v2 is `/usr/local/bin/containerd-shim-kata-v2`.
<details>
<summary><tt>/usr/local/bin/containerd-shim-kata-v2 --version</tt></summary>
<p>

```
Kata Containers containerd shim (Golang): id: "io.containerd.kata.v2", version: 3.20.0, commit: c980b6e191e174053681fb30817736e040554c10
```

</p>
</details>

---


</p>
</details>
<details>
<summary>KSM throttler</summary>
<p>


# KSM throttler


## version


## systemd service


</p>
</details>
<details>
<summary>Image details</summary>
<p>


# Image details

```yaml
---
osbuilder:
  url: "https://github.com/kata-containers/kata-containers/tools/osbuilder"
  version: "unknown"
rootfs-creation-time: "2025-08-20T17:42:31.929280580+0000Z"
description: "osbuilder rootfs"
file-format-version: "0.0.2"
architecture: "x86_64"
base-distro:
  name: "ubuntu"
  version: "noble"
  packages:
    default:
      - "chrony"
      - "dbus"
      - "init"
      - "iptables"
      - "libseccomp2"
    extra:

agent:
  url: "https://github.com/kata-containers/kata-containers"
  name: "kata-agent"
  version: "3.20.0"
  agent-is-init-daemon: "no"
```

---


</p>
</details>
<details>
<summary>Initrd details</summary>
<p>


# Initrd details

No initrd

---


</p>
</details>
<details>
<summary>Logfiles</summary>
<p>


# Logfiles


## Runtime logs

<details>
<summary>Runtime logs</summary>
<p>

No recent runtime problems found in system journal.

</p>
</details>

## Throttler logs

<details>
<summary>Throttler logs</summary>
<p>

No recent throttler problems found in system journal.

</p>
</details>

## Kata Containerd Shim v2 logs

<details>
<summary>Kata Containerd Shim v2</summary>
<p>

Recent  problems found in system journal:
```
time="2025-09-09T05:54:35.729648995Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=29115 sandbox=8b4a70841959bf7805f8038ae4a8c59ae117c9384b4232ee7df489808b466755 share-dir=/run/kata-containers/shared/sandboxes/8b4a70841959bf7805f8038ae4a8c59ae117c9384b4232ee7df489808b466755/mounts/0572bcf2b8751149e694e59c5d3f16cbe8adc393a65962ee754ee03ab01f3a03/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T05:54:35.73027722Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=29448 sandbox=00eb2a24e4fad02eef42a073e0cc75449150cde12d9057ecd9c99ffc907816bc share-dir=/run/kata-containers/shared/sandboxes/00eb2a24e4fad02eef42a073e0cc75449150cde12d9057ecd9c99ffc907816bc/mounts/af00c140fae0b557fca5763e7cf7874105fc7d55ac9dd7880d5e2d35f054d5a7/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T05:54:35.730895503Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=29342 sandbox=3b8e4dd2de957bf7eb57379d3b08790f8f4dda23964ee86d60b4cb0384d5d52f share-dir=/run/kata-containers/shared/sandboxes/3b8e4dd2de957bf7eb57379d3b08790f8f4dda23964ee86d60b4cb0384d5d52f/mounts/7e90b0e461b36af699ea6a36d61e1b3440df4bbe0296497128c5c0bb45d64bb5/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T05:54:35.746316059Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=29913 sandbox=efe85edb46b0d3f7c168abd2035748d96dd97c8c19b23288c2abc5b1f7625d60 share-dir=/run/kata-containers/shared/sandboxes/efe85edb46b0d3f7c168abd2035748d96dd97c8c19b23288c2abc5b1f7625d60/mounts/85bbc030b26eb7fa815eedb25c2f0e769d37a883f96c5cfe76dda365784c3019/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T05:54:35.749871506Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=29951 sandbox=ce6c73252d3363bef8e6833cd2959522a1d9005ca1582102ce3ae5f196f14237 share-dir=/run/kata-containers/shared/sandboxes/ce6c73252d3363bef8e6833cd2959522a1d9005ca1582102ce3ae5f196f14237/mounts/ce6c73252d3363bef8e6833cd2959522a1d9005ca1582102ce3ae5f196f14237/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T05:54:35.751830441Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=29669 sandbox=90edda945fbedc21a2ccf11f73b33003c50c3ec174ca40cef70735c5d6a86837 share-dir=/run/kata-containers/shared/sandboxes/90edda945fbedc21a2ccf11f73b33003c50c3ec174ca40cef70735c5d6a86837/mounts/d4616b69e54f1d1d6a0a7833c49c8afce7a149f73f9839ec7581bc7a356e4896/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T05:54:35.762335195Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=31259 sandbox=9be2efbdca362cd829aeb7ac376502f3bd75acc31df58e43713c6681726758d4 share-dir=/run/kata-containers/shared/sandboxes/9be2efbdca362cd829aeb7ac376502f3bd75acc31df58e43713c6681726758d4/mounts/9be2efbdca362cd829aeb7ac376502f3bd75acc31df58e43713c6681726758d4/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T05:54:35.774345367Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=28970 sandbox=dd45b53e55312985eae04ca0264a0f5beca28fb8b14511a6c87f2d8cff63a715 share-dir=/run/kata-containers/shared/sandboxes/dd45b53e55312985eae04ca0264a0f5beca28fb8b14511a6c87f2d8cff63a715/mounts/dd45b53e55312985eae04ca0264a0f5beca28fb8b14511a6c87f2d8cff63a715/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T05:54:35.783291633Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=29304 sandbox=bcccceabbaf96dc5988d11a8569ade7cc2dc294654e434b302b9c541e28a1306 share-dir=/run/kata-containers/shared/sandboxes/bcccceabbaf96dc5988d11a8569ade7cc2dc294654e434b302b9c541e28a1306/mounts/bcccceabbaf96dc5988d11a8569ade7cc2dc294654e434b302b9c541e28a1306/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T05:54:35.797327013Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=29413 sandbox=7ae5f162a4d273ba776db1c27d557968911a2d201da965330715d569afaa3b07 share-dir=/run/kata-containers/shared/sandboxes/7ae5f162a4d273ba776db1c27d557968911a2d201da965330715d569afaa3b07/mounts/7ae5f162a4d273ba776db1c27d557968911a2d201da965330715d569afaa3b07/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T05:54:35.856201936Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=29266 sandbox=805d14e84fb81e0d0e387f3fdaa3a32f5de6c5c83fab6efcccf96a7e344e36ff share-dir=/run/kata-containers/shared/sandboxes/805d14e84fb81e0d0e387f3fdaa3a32f5de6c5c83fab6efcccf96a7e344e36ff/mounts/805d14e84fb81e0d0e387f3fdaa3a32f5de6c5c83fab6efcccf96a7e344e36ff/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T05:54:35.861100586Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=29044 sandbox=9d3c53bf174067590fcdc4c47bae02d282e7f828053dfad24c76d611f5f3c7f7 share-dir=/run/kata-containers/shared/sandboxes/9d3c53bf174067590fcdc4c47bae02d282e7f828053dfad24c76d611f5f3c7f7/mounts/9d3c53bf174067590fcdc4c47bae02d282e7f828053dfad24c76d611f5f3c7f7/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T05:54:35.875229924Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=29230 sandbox=6cf085ae0547b5693022b1512ab02f373f9833e080006d6775fb9359e4e725ff share-dir=/run/kata-containers/shared/sandboxes/6cf085ae0547b5693022b1512ab02f373f9833e080006d6775fb9359e4e725ff/mounts/6cf085ae0547b5693022b1512ab02f373f9833e080006d6775fb9359e4e725ff/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T05:54:35.918676366Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=29742 sandbox=ce89ba0d3374ea1182f116ae52801f5227a59408a0d8c1632a229f1823c05b90 share-dir=/run/kata-containers/shared/sandboxes/ce89ba0d3374ea1182f116ae52801f5227a59408a0d8c1632a229f1823c05b90/mounts/ce89ba0d3374ea1182f116ae52801f5227a59408a0d8c1632a229f1823c05b90/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T05:54:35.92253415Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=29448 sandbox=00eb2a24e4fad02eef42a073e0cc75449150cde12d9057ecd9c99ffc907816bc share-dir=/run/kata-containers/shared/sandboxes/00eb2a24e4fad02eef42a073e0cc75449150cde12d9057ecd9c99ffc907816bc/mounts/00eb2a24e4fad02eef42a073e0cc75449150cde12d9057ecd9c99ffc907816bc/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T05:54:35.922807703Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=29115 sandbox=8b4a70841959bf7805f8038ae4a8c59ae117c9384b4232ee7df489808b466755 share-dir=/run/kata-containers/shared/sandboxes/8b4a70841959bf7805f8038ae4a8c59ae117c9384b4232ee7df489808b466755/mounts/8b4a70841959bf7805f8038ae4a8c59ae117c9384b4232ee7df489808b466755/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T05:54:35.928219625Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=29596 sandbox=4c1870d467c1eacadaadf52ae40c78f8593a530b33b422dcdcbe90a02f0eeb3f share-dir=/run/kata-containers/shared/sandboxes/4c1870d467c1eacadaadf52ae40c78f8593a530b33b422dcdcbe90a02f0eeb3f/mounts/87f9cfe0fc4d776a3c813c732fde2e0b33d4ab7efc8ad3de91d5aea2f9b2b3dc/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T05:54:35.938469299Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=29669 sandbox=90edda945fbedc21a2ccf11f73b33003c50c3ec174ca40cef70735c5d6a86837 share-dir=/run/kata-containers/shared/sandboxes/90edda945fbedc21a2ccf11f73b33003c50c3ec174ca40cef70735c5d6a86837/mounts/90edda945fbedc21a2ccf11f73b33003c50c3ec174ca40cef70735c5d6a86837/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T05:54:35.955372209Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=29829 sandbox=1c8014c504105867d82f51e1dfe184d38c321446d36a330d1ec351de2b29b458 share-dir=/run/kata-containers/shared/sandboxes/1c8014c504105867d82f51e1dfe184d38c321446d36a330d1ec351de2b29b458/mounts/1c8014c504105867d82f51e1dfe184d38c321446d36a330d1ec351de2b29b458/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T05:54:35.973158807Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=29342 sandbox=3b8e4dd2de957bf7eb57379d3b08790f8f4dda23964ee86d60b4cb0384d5d52f share-dir=/run/kata-containers/shared/sandboxes/3b8e4dd2de957bf7eb57379d3b08790f8f4dda23964ee86d60b4cb0384d5d52f/mounts/3b8e4dd2de957bf7eb57379d3b08790f8f4dda23964ee86d60b4cb0384d5d52f/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T05:54:35.973887587Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=30310 sandbox=de771b907f8008a0a1973292ba76809992c206f1814064eb2be496ab56b2d1c4 share-dir=/run/kata-containers/shared/sandboxes/de771b907f8008a0a1973292ba76809992c206f1814064eb2be496ab56b2d1c4/mounts/de771b907f8008a0a1973292ba76809992c206f1814064eb2be496ab56b2d1c4/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T05:54:35.980458651Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=30116 sandbox=32113fb8efa6816b2ae7978ecb17e7fc2c27e0c1f677fa84c2189c80428f896c share-dir=/run/kata-containers/shared/sandboxes/32113fb8efa6816b2ae7978ecb17e7fc2c27e0c1f677fa84c2189c80428f896c/mounts/2feedf6a322ed6ea88c3fd4d476a4a009df0a25025e332ff590e781fa5637482/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T05:54:35.980450341Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=29557 sandbox=643792f9f36cf570d66972fee7ecea93b3b55b8bcf11230b3792510712efa791 share-dir=/run/kata-containers/shared/sandboxes/643792f9f36cf570d66972fee7ecea93b3b55b8bcf11230b3792510712efa791/mounts/643792f9f36cf570d66972fee7ecea93b3b55b8bcf11230b3792510712efa791/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T05:54:36.011077755Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=29913 sandbox=efe85edb46b0d3f7c168abd2035748d96dd97c8c19b23288c2abc5b1f7625d60 share-dir=/run/kata-containers/shared/sandboxes/efe85edb46b0d3f7c168abd2035748d96dd97c8c19b23288c2abc5b1f7625d60/mounts/efe85edb46b0d3f7c168abd2035748d96dd97c8c19b23288c2abc5b1f7625d60/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T05:54:36.121049782Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=29596 sandbox=4c1870d467c1eacadaadf52ae40c78f8593a530b33b422dcdcbe90a02f0eeb3f share-dir=/run/kata-containers/shared/sandboxes/4c1870d467c1eacadaadf52ae40c78f8593a530b33b422dcdcbe90a02f0eeb3f/mounts/4c1870d467c1eacadaadf52ae40c78f8593a530b33b422dcdcbe90a02f0eeb3f/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T05:54:36.172946828Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=30116 sandbox=32113fb8efa6816b2ae7978ecb17e7fc2c27e0c1f677fa84c2189c80428f896c share-dir=/run/kata-containers/shared/sandboxes/32113fb8efa6816b2ae7978ecb17e7fc2c27e0c1f677fa84c2189c80428f896c/mounts/32113fb8efa6816b2ae7978ecb17e7fc2c27e0c1f677fa84c2189c80428f896c/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T12:11:04.052132545Z" level=warning msg="Could not add /dev/mshv to the devices cgroup" name=containerd-shim-v2 pid=198618 sandbox=71ace8fa8c42257b4861346e4d88fe6d9bfc6ef283dec02bd171e57c24c79818 source=cgroups
time="2025-09-09T12:11:04.448306921Z" level=warning msg="Could not add /dev/mshv to the devices cgroup" name=containerd-shim-v2 pid=198798 sandbox=0125348fa497fd078e8a6ee6f60f5d12dd8675445864077704e47f82c48cd008 source=cgroups
time="2025-09-09T12:11:05.069537629Z" level=warning msg="Advanced PCIe Topology only available for QEMU/CLH hypervisor, ignoring hot(cold)_vfio_port setting" name=containerd-shim-v2 pid=198908 sandbox=7dac50f9c407f76606390664222cc3850017ed7d1ddd2bcf66dcdbb9460540c6 source=katautils
time="2025-09-09T12:11:05.071274682Z" level=warning msg="Could not add /dev/mshv to the devices cgroup" name=containerd-shim-v2 pid=198908 sandbox=7dac50f9c407f76606390664222cc3850017ed7d1ddd2bcf66dcdbb9460540c6 source=cgroups
time="2025-09-09T12:11:05.096923145Z" level=error msg="getting vm status failed" error="Get \"http://localhost/\": dial unix /run/vc/firecracker/7dac50f9c407f76606390664222cc385/root/run/firecracker.socket: connect: no such file or directory" name=containerd-shim-v2 pid=198908 sandbox=7dac50f9c407f76606390664222cc3850017ed7d1ddd2bcf66dcdbb9460540c6 source=virtcontainers subsystem=firecracker
time="2025-09-09T12:11:10.540966764Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=198908 sandbox=7dac50f9c407f76606390664222cc3850017ed7d1ddd2bcf66dcdbb9460540c6 share-dir=/run/kata-containers/shared/sandboxes/7dac50f9c407f76606390664222cc3850017ed7d1ddd2bcf66dcdbb9460540c6/mounts/b5ecab422a9cf0b061c54bac2aaad14d8a0fabc4fdeb11210ae374f1a7a5222d/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T12:11:12.483795861Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=198908 sandbox=7dac50f9c407f76606390664222cc3850017ed7d1ddd2bcf66dcdbb9460540c6 share-dir=/run/kata-containers/shared/sandboxes/7dac50f9c407f76606390664222cc3850017ed7d1ddd2bcf66dcdbb9460540c6/mounts/7dac50f9c407f76606390664222cc3850017ed7d1ddd2bcf66dcdbb9460540c6/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T12:11:13.974444821Z" level=warning msg="Advanced PCIe Topology only available for QEMU/CLH hypervisor, ignoring hot(cold)_vfio_port setting" name=containerd-shim-v2 pid=199386 sandbox=bde294ff9e9516f73bf6eb055df805e0dd6299107cbe54d976dacf18c11de27b source=katautils
time="2025-09-09T12:11:13.975365222Z" level=warning msg="Could not add /dev/mshv to the devices cgroup" name=containerd-shim-v2 pid=199386 sandbox=bde294ff9e9516f73bf6eb055df805e0dd6299107cbe54d976dacf18c11de27b source=cgroups
time="2025-09-09T12:11:13.999982047Z" level=error msg="getting vm status failed" error="Get \"http://localhost/\": dial unix /run/vc/firecracker/bde294ff9e9516f73bf6eb055df805e0/root/run/firecracker.socket: connect: no such file or directory" name=containerd-shim-v2 pid=199386 sandbox=bde294ff9e9516f73bf6eb055df805e0dd6299107cbe54d976dacf18c11de27b source=virtcontainers subsystem=firecracker
time="2025-09-09T12:11:21.27316254Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=199386 sandbox=bde294ff9e9516f73bf6eb055df805e0dd6299107cbe54d976dacf18c11de27b share-dir=/run/kata-containers/shared/sandboxes/bde294ff9e9516f73bf6eb055df805e0dd6299107cbe54d976dacf18c11de27b/mounts/630e53c1d14ce3ab5f77d4678ed1fa2a788b3029a26714f83f6ed766fc48b155/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T12:11:21.536600816Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=199386 sandbox=bde294ff9e9516f73bf6eb055df805e0dd6299107cbe54d976dacf18c11de27b share-dir=/run/kata-containers/shared/sandboxes/bde294ff9e9516f73bf6eb055df805e0dd6299107cbe54d976dacf18c11de27b/mounts/bde294ff9e9516f73bf6eb055df805e0dd6299107cbe54d976dacf18c11de27b/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T12:11:23.667838118Z" level=error msg="failed to cleanup the &{%!s(*v2.Manager=&{/sys/fs/cgroup /sys/fs/cgroup/kubepods-burstable-pod1ed6311c_f336_4509_be0e_c222e22953c1.slice:cri-containerd:0125348fa497fd078e8a6ee6f60f5d12dd8675445864077704e47f82c48cd008}) /kubepods-burstable-pod1ed6311c_f336_4509_be0e_c222e22953c1.slice:cri-containerd:0125348fa497fd078e8a6ee6f60f5d12dd8675445864077704e47f82c48cd008 %!s(*specs.LinuxCPU=&{<nil> <nil> <nil> <nil> <nil> <nil>   <nil>}) [{%!s(bool=false)  %!s(*int64=<nil>) %!s(*int64=<nil>) rwm} {%!s(bool=true) c %!s(*int64=0xc000303c18) %!s(*int64=0xc000303c40) rwm} {%!s(bool=true) c %!s(*int64=0xc000303c48) %!s(*int64=0xc000303c50) rwm} {%!s(bool=true) c %!s(*int64=0xc0004674d8) %!s(*int64=0xc0004674e0) rwm} {%!s(bool=true) c %!s(*int64=0xc000467508) %!s(*int64=0xc000467510) rwm} {%!s(bool=true) c %!s(*int64=0xc000467538) %!s(*int64=0xc000467540) rwm} {%!s(bool=true) c %!s(*int64=0xc000467568) %!s(*int64=0xc000467570) rwm} {%!s(bool=true) c %!s(*int64=0xc000467598) %!s(*int64=0xc0004675a0) rwm} {%!s(bool=true) c %!s(*int64=0xc0004675c8) %!s(*int64=0xc0004675d0) rwm} {%!s(bool=true) c %!s(*int64=0xc0004675f8) %!s(*int64=0xc000467600) rwm} {%!s(bool=true) c %!s(*int64=0xc000467628) %!s(*int64=0xc000467630) rwm} {%!s(bool=true) c %!s(*int64=0xc000467748) %!s(*int64=0xc000467750) rwm} {%!s(bool=true) c %!s(*int64=0xc000467778) %!s(*int64=0xc000467780) rwm} {%!s(bool=true) c %!s(*int64=0xc0004677a8) %!s(*int64=0xc0004677b0) rwm} {%!s(bool=true) c %!s(*int64=0xc000303e28) %!s(*int64=0xc000303e40) m} {%!s(bool=true) b %!s(*int64=0xc000303e28) %!s(*int64=0xc000303e40) m} {%!s(bool=true) c %!s(*int64=0xc000303e48) %!s(*int64=0xc000303e40) rwm} {%!s(bool=true) c %!s(*int64=0xc000303e50) %!s(*int64=0xc000303e58) rwm}] %!s(bool=false) {%!s(int32=0) %!s(uint32=0)}} resource controllers" error="cgroups: unable to remove path \"/sys/fs/cgroup/kubepods-burstable-pod1ed6311c_f336_4509_be0e_c222e22953c1.slice:cri-containerd:0125348fa497fd078e8a6ee6f60f5d12dd8675445864077704e47f82c48cd008\": unlinkat /sys/fs/cgroup/kubepods-burstable-pod1ed6311c_f336_4509_be0e_c222e22953c1.slice:cri-containerd:0125348fa497fd078e8a6ee6f60f5d12dd8675445864077704e47f82c48cd008/misc.events: operation not permitted" name=containerd-shim-v2 pid=198798 sandbox=0125348fa497fd078e8a6ee6f60f5d12dd8675445864077704e47f82c48cd008 source=virtcontainers subsystem=sandbox
time="2025-09-09T12:11:24.51371235Z" level=error msg="failed to cleanup the &{%!s(*v2.Manager=&{/sys/fs/cgroup /sys/fs/cgroup/kubepods-burstable-podcac00a06_1d5c_4508_88fc_80fca91107e6.slice:cri-containerd:71ace8fa8c42257b4861346e4d88fe6d9bfc6ef283dec02bd171e57c24c79818}) /kubepods-burstable-podcac00a06_1d5c_4508_88fc_80fca91107e6.slice:cri-containerd:71ace8fa8c42257b4861346e4d88fe6d9bfc6ef283dec02bd171e57c24c79818 %!s(*specs.LinuxCPU=&{<nil> <nil> <nil> <nil> <nil> <nil>   <nil>}) [{%!s(bool=false)  %!s(*int64=<nil>) %!s(*int64=<nil>) rwm} {%!s(bool=true) c %!s(*int64=0xc000353ce8) %!s(*int64=0xc000353d20) rwm} {%!s(bool=true) c %!s(*int64=0xc000353d28) %!s(*int64=0xc000353d30) rwm} {%!s(bool=true) c %!s(*int64=0xc000236068) %!s(*int64=0xc000236070) rwm} {%!s(bool=true) c %!s(*int64=0xc000236098) %!s(*int64=0xc0002360a0) rwm} {%!s(bool=true) c %!s(*int64=0xc0002360c8) %!s(*int64=0xc0002360d0) rwm} {%!s(bool=true) c %!s(*int64=0xc0002360f8) %!s(*int64=0xc000236100) rwm} {%!s(bool=true) c %!s(*int64=0xc000236128) %!s(*int64=0xc000236130) rwm} {%!s(bool=true) c %!s(*int64=0xc000236158) %!s(*int64=0xc000236160) rwm} {%!s(bool=true) c %!s(*int64=0xc000236188) %!s(*int64=0xc000236190) rwm} {%!s(bool=true) c %!s(*int64=0xc0002361b8) %!s(*int64=0xc0002361c0) rwm} {%!s(bool=true) c %!s(*int64=0xc0002362d8) %!s(*int64=0xc0002362e0) rwm} {%!s(bool=true) c %!s(*int64=0xc000236308) %!s(*int64=0xc000236310) rwm} {%!s(bool=true) c %!s(*int64=0xc000236338) %!s(*int64=0xc000236340) rwm} {%!s(bool=true) c %!s(*int64=0xc000014298) %!s(*int64=0xc0000143f0) m} {%!s(bool=true) b %!s(*int64=0xc000014298) %!s(*int64=0xc0000143f0) m} {%!s(bool=true) c %!s(*int64=0xc0000143f8) %!s(*int64=0xc0000143f0) rwm} {%!s(bool=true) c %!s(*int64=0xc000014420) %!s(*int64=0xc000014428) rwm}] %!s(bool=false) {%!s(int32=0) %!s(uint32=0)}} resource controllers" error="cgroups: unable to remove path \"/sys/fs/cgroup/kubepods-burstable-podcac00a06_1d5c_4508_88fc_80fca91107e6.slice:cri-containerd:71ace8fa8c42257b4861346e4d88fe6d9bfc6ef283dec02bd171e57c24c79818\": still contains running processes" name=containerd-shim-v2 pid=198618 sandbox=71ace8fa8c42257b4861346e4d88fe6d9bfc6ef283dec02bd171e57c24c79818 source=virtcontainers subsystem=sandbox
time="2025-09-09T12:11:25.09196884Z" level=warning msg="Advanced PCIe Topology only available for QEMU/CLH hypervisor, ignoring hot(cold)_vfio_port setting" name=containerd-shim-v2 pid=200061 sandbox=f4ac825efde0220fa763cb78ccea3ed3d445fc1abc5686953e4de8ff7e7aa53f source=katautils
time="2025-09-09T12:11:25.09280743Z" level=warning msg="Could not add /dev/mshv to the devices cgroup" name=containerd-shim-v2 pid=200061 sandbox=f4ac825efde0220fa763cb78ccea3ed3d445fc1abc5686953e4de8ff7e7aa53f source=cgroups
time="2025-09-09T12:11:25.122466773Z" level=error msg="getting vm status failed" error="Get \"http://localhost/\": dial unix /run/vc/firecracker/f4ac825efde0220fa763cb78ccea3ed3/root/run/firecracker.socket: connect: no such file or directory" name=containerd-shim-v2 pid=200061 sandbox=f4ac825efde0220fa763cb78ccea3ed3d445fc1abc5686953e4de8ff7e7aa53f source=virtcontainers subsystem=firecracker
time="2025-09-09T12:11:25.770227706Z" level=warning msg="Advanced PCIe Topology only available for QEMU/CLH hypervisor, ignoring hot(cold)_vfio_port setting" name=containerd-shim-v2 pid=200136 sandbox=99bf3b591ea5757be606f27ed7b5c9b5cda67fb653413d0d158dce8b70bfeae4 source=katautils
time="2025-09-09T12:11:25.771103831Z" level=warning msg="Could not add /dev/mshv to the devices cgroup" name=containerd-shim-v2 pid=200136 sandbox=99bf3b591ea5757be606f27ed7b5c9b5cda67fb653413d0d158dce8b70bfeae4 source=cgroups
time="2025-09-09T12:11:25.792415191Z" level=error msg="getting vm status failed" error="Get \"http://localhost/\": dial unix /run/vc/firecracker/99bf3b591ea5757be606f27ed7b5c9b5/root/run/firecracker.socket: connect: no such file or directory" name=containerd-shim-v2 pid=200136 sandbox=99bf3b591ea5757be606f27ed7b5c9b5cda67fb653413d0d158dce8b70bfeae4 source=virtcontainers subsystem=firecracker
time="2025-09-09T12:11:26.290475666Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=200061 sandbox=f4ac825efde0220fa763cb78ccea3ed3d445fc1abc5686953e4de8ff7e7aa53f share-dir=/run/kata-containers/shared/sandboxes/f4ac825efde0220fa763cb78ccea3ed3d445fc1abc5686953e4de8ff7e7aa53f/mounts/2251433a4dacd4478015a3b4f9f9863ab7817e51595d65107058b0c12d426d53/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T12:11:26.793479571Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=200136 sandbox=99bf3b591ea5757be606f27ed7b5c9b5cda67fb653413d0d158dce8b70bfeae4 share-dir=/run/kata-containers/shared/sandboxes/99bf3b591ea5757be606f27ed7b5c9b5cda67fb653413d0d158dce8b70bfeae4/mounts/b9ec5ae54abcbb6a9ee0f00980f5c20c6a3e5ab5e85d3b313dbd4558d5c43f27/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T12:11:28.490458055Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=200136 sandbox=99bf3b591ea5757be606f27ed7b5c9b5cda67fb653413d0d158dce8b70bfeae4 share-dir=/run/kata-containers/shared/sandboxes/99bf3b591ea5757be606f27ed7b5c9b5cda67fb653413d0d158dce8b70bfeae4/mounts/99bf3b591ea5757be606f27ed7b5c9b5cda67fb653413d0d158dce8b70bfeae4/rootfs source=virtcontainers subsystem=mount
time="2025-09-09T12:11:28.569232361Z" level=warning error="no such file or directory" name=containerd-shim-v2 pid=200061 sandbox=f4ac825efde0220fa763cb78ccea3ed3d445fc1abc5686953e4de8ff7e7aa53f share-dir=/run/kata-containers/shared/sandboxes/f4ac825efde0220fa763cb78ccea3ed3d445fc1abc5686953e4de8ff7e7aa53f/mounts/f4ac825efde0220fa763cb78ccea3ed3d445fc1abc5686953e4de8ff7e7aa53f/rootfs source=virtcontainers subsystem=mount
```

</p>
</details>

---


</p>
</details>
<details>
<summary>Container manager details</summary>
<p>


# Container manager details

<details>
<summary>Kubernetes</summary>
<p>


## Kubernetes

<details>
<summary><tt>kubectl version</tt></summary>
<p>

```
Client Version: v1.33.2
Kustomize Version: v5.6.0
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```

</p>
</details>
<details>
<summary><tt>kubectl config view</tt></summary>
<p>

```
apiVersion: v1
clusters: null
contexts: null
current-context: ""
kind: Config
preferences: {}
users: null
```

</p>
</details>
<details>
<summary><tt>systemctl show kubelet</tt></summary>
<p>

```
Type=simple
ExitType=main
Restart=on-failure
RestartMode=normal
NotifyAccess=none
RestartUSec=5s
RestartSteps=0
RestartMaxDelayUSec=infinity
RestartUSecNext=5s
TimeoutStartUSec=1min 30s
TimeoutStopUSec=1min 30s
TimeoutAbortUSec=1min 30s
TimeoutStartFailureMode=terminate
TimeoutStopFailureMode=terminate
RuntimeMaxUSec=infinity
RuntimeRandomizedExtraUSec=0
WatchdogUSec=0
WatchdogTimestampMonotonic=0
RootDirectoryStartOnly=no
RemainAfterExit=no
GuessMainPID=yes
RestartForceExitStatus=PIPE
MainPID=2330
ControlPID=0
FileDescriptorStoreMax=0
NFileDescriptorStore=0
FileDescriptorStorePreserve=restart
StatusErrno=0
Result=success
ReloadResult=success
CleanResult=success
UID=[not set]
GID=[not set]
NRestarts=0
OOMPolicy=stop
ReloadSignal=1
ExecMainStartTimestamp=Tue 2025-09-09 04:50:52 UTC
ExecMainStartTimestampMonotonic=16119225
ExecMainExitTimestampMonotonic=0
ExecMainPID=2330
ExecMainCode=0
ExecMainStatus=0
ExecStartPre={ path=/usr/sbin/iptables ; argv[]=/usr/sbin/iptables -P FORWARD ACCEPT -w 5 ; ignore_errors=no ; start_time=[Tue 2025-09-09 04:50:52 UTC] ; stop_time=[Tue 2025-09-09 04:50:52 UTC] ; pid=2327 ; code=exited ; status=0 }
ExecStartPreEx={ path=/usr/sbin/iptables ; argv[]=/usr/sbin/iptables -P FORWARD ACCEPT -w 5 ; flags= ; start_time=[Tue 2025-09-09 04:50:52 UTC] ; stop_time=[Tue 2025-09-09 04:50:52 UTC] ; pid=2327 ; code=exited ; status=0 }
ExecStart={ path=/usr/bin/kubelet ; argv[]=/usr/bin/kubelet $NODEADM_KUBELET_ARGS ; ignore_errors=no ; start_time=[Tue 2025-09-09 04:50:52 UTC] ; stop_time=[n/a] ; pid=2330 ; code=(null) ; status=0/0 }
ExecStartEx={ path=/usr/bin/kubelet ; argv[]=/usr/bin/kubelet $NODEADM_KUBELET_ARGS ; flags= ; start_time=[Tue 2025-09-09 04:50:52 UTC] ; stop_time=[n/a] ; pid=2330 ; code=(null) ; status=0/0 }
Slice=runtime.slice
ControlGroup=/runtime.slice/kubelet.service
ControlGroupId=4221
MemoryCurrent=180961280
MemoryPeak=287920128
MemorySwapCurrent=0
MemorySwapPeak=0
MemoryZSwapCurrent=0
MemoryAvailable=151242424320
CPUUsageNSec=13518271787000
EffectiveCPUs=0-95
EffectiveMemoryNodes=0-1
TasksCurrent=104
IPIngressBytes=[no data]
IPIngressPackets=[no data]
IPEgressBytes=[no data]
IPEgressPackets=[no data]
IOReadBytes=[not set]
IOReadOperations=[not set]
IOWriteBytes=[not set]
IOWriteOperations=[not set]
Delegate=no
CPUAccounting=yes
CPUWeight=[not set]
StartupCPUWeight=[not set]
CPUShares=[not set]
StartupCPUShares=[not set]
CPUQuotaPerSecUSec=infinity
CPUQuotaPeriodUSec=infinity
IOAccounting=no
IOWeight=[not set]
StartupIOWeight=[not set]
BlockIOAccounting=no
BlockIOWeight=[not set]
StartupBlockIOWeight=[not set]
MemoryAccounting=yes
DefaultMemoryLow=0
DefaultStartupMemoryLow=0
DefaultMemoryMin=0
MemoryMin=0
MemoryLow=0
StartupMemoryLow=0
MemoryHigh=infinity
StartupMemoryHigh=infinity
MemoryMax=infinity
StartupMemoryMax=infinity
MemorySwapMax=infinity
StartupMemorySwapMax=infinity
MemoryZSwapMax=infinity
StartupMemoryZSwapMax=infinity
MemoryLimit=infinity
DevicePolicy=auto
TasksAccounting=yes
TasksMax=231598
IPAccounting=no
ManagedOOMSwap=auto
ManagedOOMMemoryPressure=auto
ManagedOOMMemoryPressureLimit=0
ManagedOOMPreference=none
MemoryPressureWatch=auto
MemoryPressureThresholdUSec=200ms
CoredumpReceive=no
EnvironmentFiles=/etc/eks/kubelet/environment (ignore_errors=no)
UMask=0022
LimitCPU=infinity
LimitCPUSoft=infinity
LimitFSIZE=infinity
LimitFSIZESoft=infinity
LimitDATA=infinity
LimitDATASoft=infinity
LimitSTACK=infinity
LimitSTACKSoft=8388608
LimitCORE=infinity
LimitCORESoft=0
LimitRSS=infinity
LimitRSSSoft=infinity
LimitNOFILE=524288
LimitNOFILESoft=1024
LimitAS=infinity
LimitASSoft=infinity
LimitNPROC=771996
LimitNPROCSoft=771996
LimitMEMLOCK=8388608
LimitMEMLOCKSoft=8388608
LimitLOCKS=infinity
LimitLOCKSSoft=infinity
LimitSIGPENDING=771996
LimitSIGPENDINGSoft=771996
LimitMSGQUEUE=819200
LimitMSGQUEUESoft=819200
LimitNICE=0
LimitNICESoft=0
LimitRTPRIO=0
LimitRTPRIOSoft=0
LimitRTTIME=infinity
LimitRTTIMESoft=infinity
RootEphemeral=no
OOMScoreAdjust=0
CoredumpFilter=0x33
Nice=0
IOSchedulingClass=2
IOSchedulingPriority=4
CPUSchedulingPolicy=0
CPUSchedulingPriority=0
CPUAffinityFromNUMA=no
NUMAPolicy=n/a
TimerSlackNSec=50000
CPUSchedulingResetOnFork=no
NonBlocking=no
StandardInput=null
StandardOutput=journal
StandardError=inherit
TTYReset=no
TTYVHangup=no
TTYVTDisallocate=no
SyslogPriority=30
SyslogLevelPrefix=yes
SyslogLevel=6
SyslogFacility=3
LogLevelMax=-1
LogRateLimitIntervalUSec=0
LogRateLimitBurst=0
SecureBits=0
CapabilityBoundingSet=cap_chown cap_dac_override cap_dac_read_search cap_fowner cap_fsetid cap_kill cap_setgid cap_setuid cap_setpcap cap_linux_immutable cap_net_bind_service cap_net_broadcast cap_net_admin cap_net_raw cap_ipc_lock cap_ipc_owner cap_sys_module cap_sys_rawio cap_sys_chroot cap_sys_ptrace cap_sys_pacct cap_sys_admin cap_sys_boot cap_sys_nice cap_sys_resource cap_sys_time cap_sys_tty_config cap_mknod cap_lease cap_audit_write cap_audit_control cap_setfcap cap_mac_override cap_mac_admin cap_syslog cap_wake_alarm cap_block_suspend cap_audit_read cap_perfmon cap_bpf cap_checkpoint_restore
DynamicUser=no
SetLoginEnvironment=no
RemoveIPC=no
PrivateTmp=no
PrivateDevices=no
ProtectClock=no
ProtectKernelTunables=no
ProtectKernelModules=no
ProtectKernelLogs=no
ProtectControlGroups=no
PrivateNetwork=no
PrivateUsers=no
PrivateMounts=no
PrivateIPC=no
ProtectHome=no
ProtectSystem=no
SameProcessGroup=no
UtmpMode=init
IgnoreSIGPIPE=yes
NoNewPrivileges=no
SystemCallErrorNumber=2147483646
LockPersonality=no
RuntimeDirectoryPreserve=no
RuntimeDirectoryMode=0755
StateDirectoryMode=0755
CacheDirectoryMode=0755
LogsDirectoryMode=0755
ConfigurationDirectoryMode=0755
TimeoutCleanUSec=infinity
MemoryDenyWriteExecute=no
RestrictRealtime=no
RestrictSUIDSGID=no
RestrictNamespaces=no
MountAPIVFS=no
KeyringMode=private
ProtectProc=default
ProcSubset=all
ProtectHostname=no
MemoryKSM=no
RootImagePolicy=root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent
MountImagePolicy=root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent
ExtensionImagePolicy=root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent
KillMode=process
KillSignal=15
RestartKillSignal=15
FinalKillSignal=9
SendSIGKILL=yes
SendSIGHUP=no
WatchdogSignal=6
Id=kubelet.service
Names=kubelet.service
Requires=sysinit.target runtime.slice
Wants=containerd.service
Conflicts=shutdown.target
Before=shutdown.target
After=systemd-journald.socket runtime.slice containerd.service nodeadm-config.service basic.target sysinit.target overlaybd-credentials-refresh.service
Documentation=https://github.com/kubernetes/kubernetes
Description=Kubernetes Kubelet
LoadState=loaded
ActiveState=active
FreezerState=running
SubState=running
FragmentPath=/etc/systemd/system/kubelet.service
UnitFileState=disabled
UnitFilePreset=enabled
StateChangeTimestamp=Tue 2025-09-09 04:50:52 UTC
StateChangeTimestampMonotonic=16119790
InactiveExitTimestamp=Tue 2025-09-09 04:50:52 UTC
InactiveExitTimestampMonotonic=16075442
ActiveEnterTimestamp=Tue 2025-09-09 04:50:52 UTC
ActiveEnterTimestampMonotonic=16119335
ActiveExitTimestampMonotonic=0
InactiveEnterTimestampMonotonic=0
CanStart=yes
CanStop=yes
CanReload=no
CanIsolate=no
CanFreeze=yes
StopWhenUnneeded=no
RefuseManualStart=no
RefuseManualStop=no
AllowIsolate=no
DefaultDependencies=yes
SurviveFinalKillSignal=no
OnSuccessJobMode=fail
OnFailureJobMode=replace
IgnoreOnIsolate=no
NeedDaemonReload=no
JobTimeoutUSec=infinity
JobRunningTimeoutUSec=infinity
JobTimeoutAction=none
ConditionResult=yes
AssertResult=yes
ConditionTimestamp=Tue 2025-09-09 04:50:52 UTC
ConditionTimestampMonotonic=16048140
AssertTimestamp=Tue 2025-09-09 04:50:52 UTC
AssertTimestampMonotonic=16048142
Transient=no
Perpetual=no
StartLimitIntervalUSec=10s
StartLimitBurst=5
StartLimitAction=none
FailureAction=none
SuccessAction=none
InvocationID=ee88e86ee9b24a8b9583eb74794b68db
CollectMode=inactive
```

</p>
</details>

</p>
</details>
<details>
<summary>containerd</summary>
<p>


## containerd

<details>
<summary><tt>containerd --version</tt></summary>
<p>

```
containerd github.com/containerd/containerd/v2 v2.1.4-4-runloop 73b3916975535e8453d9fe66abf361b97fe1032d
```

</p>
</details>
<details>
<summary><tt>systemctl show containerd</tt></summary>
<p>

```
Type=notify
ExitType=main
Restart=always
RestartMode=normal
NotifyAccess=main
RestartUSec=5s
RestartSteps=0
RestartMaxDelayUSec=infinity
RestartUSecNext=5s
TimeoutStartUSec=1min 30s
TimeoutStopUSec=1min 30s
TimeoutAbortUSec=1min 30s
TimeoutStartFailureMode=terminate
TimeoutStopFailureMode=terminate
RuntimeMaxUSec=infinity
RuntimeRandomizedExtraUSec=0
WatchdogUSec=0
WatchdogTimestampMonotonic=0
RootDirectoryStartOnly=no
RemainAfterExit=no
GuessMainPID=yes
MainPID=2295
ControlPID=0
FileDescriptorStoreMax=0
NFileDescriptorStore=0
FileDescriptorStorePreserve=restart
StatusErrno=0
Result=success
ReloadResult=success
CleanResult=success
UID=[not set]
GID=[not set]
NRestarts=0
OOMPolicy=continue
ReloadSignal=1
ExecMainStartTimestamp=Tue 2025-09-09 04:50:51 UTC
ExecMainStartTimestampMonotonic=15549461
ExecMainExitTimestampMonotonic=0
ExecMainPID=2295
ExecMainCode=0
ExecMainStatus=0
ExecStartPre={ path=/sbin/modprobe ; argv[]=/sbin/modprobe overlay ; ignore_errors=yes ; start_time=[Tue 2025-09-09 04:50:51 UTC] ; stop_time=[Tue 2025-09-09 04:50:51 UTC] ; pid=2293 ; code=exited ; status=0 }
ExecStartPreEx={ path=/sbin/modprobe ; argv[]=/sbin/modprobe overlay ; flags=ignore-failure ; start_time=[Tue 2025-09-09 04:50:51 UTC] ; stop_time=[Tue 2025-09-09 04:50:51 UTC] ; pid=2293 ; code=exited ; status=0 }
ExecStart={ path=/usr/local/bin/containerd ; argv[]=/usr/local/bin/containerd ; ignore_errors=no ; start_time=[Tue 2025-09-09 04:50:51 UTC] ; stop_time=[n/a] ; pid=2295 ; code=(null) ; status=0/0 }
ExecStartEx={ path=/usr/local/bin/containerd ; argv[]=/usr/local/bin/containerd ; flags= ; start_time=[Tue 2025-09-09 04:50:51 UTC] ; stop_time=[n/a] ; pid=2295 ; code=(null) ; status=0/0 }
Slice=runtime.slice
ControlGroup=/runtime.slice/containerd.service
ControlGroupId=4160
MemoryCurrent=49721532416
MemoryPeak=49770123264
MemorySwapCurrent=0
MemorySwapPeak=0
MemoryZSwapCurrent=0
MemoryAvailable=151231590400
CPUUsageNSec=6441419065000
EffectiveCPUs=0-95
EffectiveMemoryNodes=0-1
TasksCurrent=418
IPIngressBytes=[no data]
IPIngressPackets=[no data]
IPEgressBytes=[no data]
IPEgressPackets=[no data]
IOReadBytes=[not set]
IOReadOperations=[not set]
IOWriteBytes=[not set]
IOWriteOperations=[not set]
Delegate=yes
DelegateControllers=cpu cpuset io memory pids
CPUAccounting=yes
CPUWeight=[not set]
StartupCPUWeight=[not set]
CPUShares=[not set]
StartupCPUShares=[not set]
CPUQuotaPerSecUSec=infinity
CPUQuotaPeriodUSec=infinity
IOAccounting=no
IOWeight=[not set]
StartupIOWeight=[not set]
BlockIOAccounting=no
BlockIOWeight=[not set]
StartupBlockIOWeight=[not set]
MemoryAccounting=yes
DefaultMemoryLow=0
DefaultStartupMemoryLow=0
DefaultMemoryMin=0
MemoryMin=0
MemoryLow=0
StartupMemoryLow=0
MemoryHigh=infinity
StartupMemoryHigh=infinity
MemoryMax=infinity
StartupMemoryMax=infinity
MemorySwapMax=infinity
StartupMemorySwapMax=infinity
MemoryZSwapMax=infinity
StartupMemoryZSwapMax=infinity
MemoryLimit=infinity
DevicePolicy=auto
TasksAccounting=yes
TasksMax=infinity
IPAccounting=no
ManagedOOMSwap=auto
ManagedOOMMemoryPressure=auto
ManagedOOMMemoryPressureLimit=0
ManagedOOMPreference=none
MemoryPressureWatch=auto
MemoryPressureThresholdUSec=200ms
CoredumpReceive=no
Environment=OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317 OTEL_EXPORTER_OTLP_PROTOCOL=grpc OTEL_SERVICE_NAME=containerd OTEL_TRACES_SAMPLER=traceidratio OTEL_TRACES_SAMPLER_ARG=1.0
UMask=0022
LimitCPU=infinity
LimitCPUSoft=infinity
LimitFSIZE=infinity
LimitFSIZESoft=infinity
LimitDATA=infinity
LimitDATASoft=infinity
LimitSTACK=infinity
LimitSTACKSoft=8388608
LimitCORE=infinity
LimitCORESoft=infinity
LimitRSS=infinity
LimitRSSSoft=infinity
LimitNOFILE=524288
LimitNOFILESoft=1024
LimitAS=infinity
LimitASSoft=infinity
LimitNPROC=infinity
LimitNPROCSoft=infinity
LimitMEMLOCK=8388608
LimitMEMLOCKSoft=8388608
LimitLOCKS=infinity
LimitLOCKSSoft=infinity
LimitSIGPENDING=771996
LimitSIGPENDINGSoft=771996
LimitMSGQUEUE=819200
LimitMSGQUEUESoft=819200
LimitNICE=0
LimitNICESoft=0
LimitRTPRIO=0
LimitRTPRIOSoft=0
LimitRTTIME=infinity
LimitRTTIMESoft=infinity
RootEphemeral=no
OOMScoreAdjust=-999
CoredumpFilter=0x33
Nice=0
IOSchedulingClass=2
IOSchedulingPriority=4
CPUSchedulingPolicy=0
CPUSchedulingPriority=0
CPUAffinityFromNUMA=no
NUMAPolicy=n/a
TimerSlackNSec=50000
CPUSchedulingResetOnFork=no
NonBlocking=no
StandardInput=null
StandardOutput=journal
StandardError=inherit
TTYReset=no
TTYVHangup=no
TTYVTDisallocate=no
SyslogPriority=30
SyslogLevelPrefix=yes
SyslogLevel=6
SyslogFacility=3
LogLevelMax=-1
LogRateLimitIntervalUSec=0
LogRateLimitBurst=0
SecureBits=0
CapabilityBoundingSet=cap_chown cap_dac_override cap_dac_read_search cap_fowner cap_fsetid cap_kill cap_setgid cap_setuid cap_setpcap cap_linux_immutable cap_net_bind_service cap_net_broadcast cap_net_admin cap_net_raw cap_ipc_lock cap_ipc_owner cap_sys_module cap_sys_rawio cap_sys_chroot cap_sys_ptrace cap_sys_pacct cap_sys_admin cap_sys_boot cap_sys_nice cap_sys_resource cap_sys_time cap_sys_tty_config cap_mknod cap_lease cap_audit_write cap_audit_control cap_setfcap cap_mac_override cap_mac_admin cap_syslog cap_wake_alarm cap_block_suspend cap_audit_read cap_perfmon cap_bpf cap_checkpoint_restore
DynamicUser=no
SetLoginEnvironment=no
RemoveIPC=no
PrivateTmp=no
PrivateDevices=no
ProtectClock=no
ProtectKernelTunables=no
ProtectKernelModules=no
ProtectKernelLogs=no
ProtectControlGroups=no
PrivateNetwork=no
PrivateUsers=no
PrivateMounts=no
PrivateIPC=no
ProtectHome=no
ProtectSystem=no
SameProcessGroup=no
UtmpMode=init
IgnoreSIGPIPE=yes
NoNewPrivileges=no
SystemCallErrorNumber=2147483646
LockPersonality=no
RuntimeDirectoryPreserve=no
RuntimeDirectoryMode=0755
StateDirectoryMode=0755
CacheDirectoryMode=0755
LogsDirectoryMode=0755
ConfigurationDirectoryMode=0755
TimeoutCleanUSec=infinity
MemoryDenyWriteExecute=no
RestrictRealtime=no
RestrictSUIDSGID=no
RestrictNamespaces=no
MountAPIVFS=no
KeyringMode=private
ProtectProc=default
ProcSubset=all
ProtectHostname=no
MemoryKSM=no
RootImagePolicy=root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent
MountImagePolicy=root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent
ExtensionImagePolicy=root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent
KillMode=process
KillSignal=15
RestartKillSignal=15
FinalKillSignal=9
SendSIGKILL=yes
SendSIGHUP=no
WatchdogSignal=6
Id=containerd.service
Names=containerd.service
Requires=sysinit.target runtime.slice
WantedBy=kubelet.service
Conflicts=shutdown.target
Before=kubelet.service shutdown.target
After=systemd-journald.socket sysinit.target basic.target dbus.service runtime.slice network.target nodeadm-config.service
Documentation=https://containerd.io
Description=containerd container runtime
LoadState=loaded
ActiveState=active
FreezerState=running
SubState=running
FragmentPath=/etc/systemd/system/containerd.service
DropInPaths=/etc/systemd/system/containerd.service.d/00-runtime-slice.conf /etc/systemd/system/containerd.service.d/05-otel-env.conf
UnitFileState=disabled
UnitFilePreset=enabled
StateChangeTimestamp=Tue 2025-09-09 04:50:51 UTC
StateChangeTimestampMonotonic=15806750
InactiveExitTimestamp=Tue 2025-09-09 04:50:51 UTC
InactiveExitTimestampMonotonic=15544776
ActiveEnterTimestamp=Tue 2025-09-09 04:50:51 UTC
ActiveEnterTimestampMonotonic=15806750
ActiveExitTimestampMonotonic=0
InactiveEnterTimestampMonotonic=0
CanStart=yes
CanStop=yes
CanReload=no
CanIsolate=no
CanFreeze=yes
StopWhenUnneeded=no
RefuseManualStart=no
RefuseManualStop=no
AllowIsolate=no
DefaultDependencies=yes
SurviveFinalKillSignal=no
OnSuccessJobMode=fail
OnFailureJobMode=replace
IgnoreOnIsolate=no
NeedDaemonReload=no
JobTimeoutUSec=infinity
JobRunningTimeoutUSec=infinity
JobTimeoutAction=none
ConditionResult=yes
AssertResult=yes
ConditionTimestamp=Tue 2025-09-09 04:50:51 UTC
ConditionTimestampMonotonic=15543928
AssertTimestamp=Tue 2025-09-09 04:50:51 UTC
AssertTimestampMonotonic=15543929
Transient=no
Perpetual=no
StartLimitIntervalUSec=10s
StartLimitBurst=5
StartLimitAction=none
FailureAction=none
SuccessAction=none
InvocationID=eb45aa468d554cb9aa1145fc7a050df4
CollectMode=inactive
```

</p>
</details>
<details>
<summary><tt>cat /etc/containerd/config.toml</tt></summary>
<p>

```toml
root = '/var/lib/containerd'
state = '/run/containerd'
version = 2

[grpc]
address = '/run/containerd/containerd.sock'

[metrics]
address = '0.0.0.0:9113'
grpc_histogram = true

[plugins]
[plugins.'io.containerd.cri.v1.images']
disable_snapshot_annotations = false
discard_unpacked_layers = false
max_concurrent_downloads = 16
use_local_image_pull = true

[plugins.'io.containerd.grpc.v1.cri']
enable_cdi = true
sandbox_image = 'registry.k8s.io/pause:3.10'

[plugins.'io.containerd.grpc.v1.cri'.cni]
bin_dir = '/opt/cni/bin'
conf_dir = '/etc/cni/net.d'

[plugins.'io.containerd.grpc.v1.cri'.containerd]
default_runtime_name = 'runc'
disable_snapshot_annotations = false
discard_unpacked_layers = false
snapshotter = 'overlaybd'
use_local_image_pull = true

[plugins.'io.containerd.grpc.v1.cri'.containerd.runtimes]
[plugins.'io.containerd.grpc.v1.cri'.containerd.runtimes.kata-clh]
pod_annotations = ['io.katacontainers.*']
privileged_without_host_devices = true
runtime_path = '/opt/kata/bin/containerd-shim-kata-v2'
runtime_type = 'io.containerd.kata-clh.v2'

[plugins.'io.containerd.grpc.v1.cri'.containerd.runtimes.kata-clh.options]
ConfigPath = '/opt/kata/share/defaults/kata-containers/configuration-clh.toml'

[plugins.'io.containerd.grpc.v1.cri'.containerd.runtimes.kata-clh-obd]
pod_annotations = ['io.katacontainers.*']
privileged_without_host_devices = true
runtime_path = '/opt/kata/bin/containerd-shim-kata-v2'
runtime_type = 'io.containerd.kata-clh.v2'
snapshotter = 'overlaybd'

[plugins.'io.containerd.grpc.v1.cri'.containerd.runtimes.kata-clh-obd.options]
ConfigPath = '/opt/kata/share/defaults/kata-containers/configuration-clh.toml'

[plugins.'io.containerd.grpc.v1.cri'.containerd.runtimes.kata-fc-obd]
pod_annotations = ['io.katacontainers.*']
privileged_without_host_devices = true
runtime_path = '/opt/kata/bin/containerd-shim-kata-v2'
runtime_type = 'io.containerd.kata-fc.v2'
snapshotter = 'overlaybd'

[plugins.'io.containerd.grpc.v1.cri'.containerd.runtimes.kata-fc-obd.options]
ConfigPath = '/opt/kata/share/defaults/kata-containers/configuration-fc.toml'

[plugins.'io.containerd.grpc.v1.cri'.containerd.runtimes.kata-qemu-obd]
pod_annotations = ['io.katacontainers.*']
privileged_without_host_devices = true
runtime_path = '/opt/kata/bin/containerd-shim-kata-v2'
runtime_type = 'io.containerd.kata-qemu.v2'
snapshotter = 'overlaybd'

[plugins.'io.containerd.grpc.v1.cri'.containerd.runtimes.kata-qemu-obd.options]
ConfigPath = '/opt/kata/share/defaults/kata-containers/configuration-qemu.toml'

[plugins.'io.containerd.grpc.v1.cri'.containerd.runtimes.runc]
base_runtime_spec = '/etc/containerd/base-runtime-spec.json'
runtime_type = 'io.containerd.runc.v2'

[plugins.'io.containerd.grpc.v1.cri'.containerd.runtimes.runc.options]
BinaryName = '/usr/sbin/runc'
SystemdCgroup = true

[plugins.'io.containerd.grpc.v1.cri'.registry]
config_path = '/etc/containerd/certs.d'

[proxy_plugins]
[proxy_plugins.overlaybd]
address = '/run/overlaybd-snapshotter/overlaybd.sock'
type = 'snapshot'

[proxy_plugins.overlaybd.exports]
enable_remote_snapshot_annotations = 'true'
```

</p>
</details>

</p>
</details>

---


</p>
</details>
<details>
<summary>Packages</summary>
<p>


# Packages

Have `dpkg`
<details>
<summary><tt>dpkg -l|grep -E "(cc-oci-runtime|cc-runtime|runv|kata-runtime|kata-ksm-throttler|kata-containers-image|linux-container|qemu-)"</tt></summary>
<p>

```

```

</p>
</details>
No `rpm`

---


</p>
</details>
<details>
<summary>Kata Monitor</summary>
<p>

Kata Monitor `kata-monitor`.
<details>
<summary><tt> --version</tt></summary>
<p>

```
/usr/local/bin/kata-collect-data.sh: line 222: --version: command not found
```

</p>
</details>

---


</p>
</details>

</p>
</details>
